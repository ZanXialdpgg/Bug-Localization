{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2d62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#\n",
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "PREPROCESSING_DIR = Path.cwd() / 'PreProcessingData'\n",
    "sys.path.append(str(PREPROCESSING_DIR))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eae219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 5 datasets:\n",
      "['aspectj', 'eclipse', 'swt', 'tomcat', 'birt']\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the filename saved by run_all_preprocessing.py\n",
    "FILENAME = Path.cwd() / 'PreProcessingData' / 'all_datasets.pkl'\n",
    "\n",
    "# 2. Open the file in binary read mode ('rb') and load the content\n",
    "with open(FILENAME, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# --- Access the data ---\n",
    "# all_processed_data is a dictionary where keys are dataset names (e.g., 'eclipse')\n",
    "print(f\"Successfully loaded {len(data)} datasets:\")\n",
    "print(list(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f993de78",
   "metadata": {},
   "source": [
    "### Remove Unusable Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2574e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_report(name):\n",
    "    rpkey = list(data[name]['report'].keys())\n",
    "    for id in rpkey:\n",
    "        count = 0\n",
    "        for f in data[name]['report'][id].fixed_files:\n",
    "            if f in data[name]['source'].keys():\n",
    "                count += 1\n",
    "                break\n",
    "        if count == 0:\n",
    "            del data[name]['report'][id]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd244be",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_report('aspectj')\n",
    "remove_report('eclipse')\n",
    "remove_report('swt')\n",
    "remove_report('tomcat')\n",
    "remove_report('birt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa559802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522 4986 3321 1012 4175\n"
     ]
    }
   ],
   "source": [
    "print(len(data['aspectj']['report']), len(data['eclipse']['report']), len(data['swt']['report']), len(data['tomcat']['report']), len(data['birt']['report']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb64f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6910 6165 2176 1794 9697\n"
     ]
    }
   ],
   "source": [
    "print(len(data['aspectj']['source']), len(data['eclipse']['source']), len(data['swt']['source']), len(data['tomcat']['source']), len(data['birt']['source']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739918f",
   "metadata": {},
   "source": [
    "# I. Temporal split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdacd592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_reports_by_time(data):\n",
    "    reports_dict = data['report']\n",
    "    # 1. Convert the dictionary items (bug_id: BugReport object) into a list of tuples\n",
    "    report_items = list(reports_dict.items())\n",
    "    # 2. Sort the list by the 'report_time' attribute of the BugReport object\n",
    "    #    We use the index [1] to access the BugReport object in the tuple (bug_id, BugReport)\n",
    "    #    and then access its .report_time attribute.\n",
    "    sorted_list = sorted(report_items,\n",
    "                        key=lambda item: item[1].report_time)\n",
    "    # 3. Store the chronologically sorted list\n",
    "    print(f\"Dataset '{data.keys}' reports sorted: {len(sorted_list)} total reports.\")\n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "491464af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset '<built-in method keys of dict object at 0x000001CB560B4900>' reports sorted: 522 total reports.\n",
      "Dataset '<built-in method keys of dict object at 0x000001CB71B06240>' reports sorted: 4986 total reports.\n",
      "Dataset '<built-in method keys of dict object at 0x000001CBAEAD2200>' reports sorted: 3321 total reports.\n",
      "Dataset '<built-in method keys of dict object at 0x000001CBD3817B00>' reports sorted: 1012 total reports.\n",
      "Dataset '<built-in method keys of dict object at 0x000001CB95643F00>' reports sorted: 4175 total reports.\n"
     ]
    }
   ],
   "source": [
    "temp_sort_aspectj = sort_reports_by_time(data['aspectj'])\n",
    "temp_sort_eclipse = sort_reports_by_time(data['eclipse'])\n",
    "temp_sort_swt = sort_reports_by_time(data['swt'])\n",
    "temp_sort_tomcat = sort_reports_by_time(data['tomcat'])\n",
    "temp_sort_birt = sort_reports_by_time(data['birt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d6bde3",
   "metadata": {},
   "source": [
    "# II. Split to 10-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3d13ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(sorted_reports, k):\n",
    "    fold_size = len(sorted_reports) // k\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        start_index = i * fold_size\n",
    "        if i == k - 1:  # Last fold takes the remainder\n",
    "            end_index = len(sorted_reports)\n",
    "        else:\n",
    "            end_index = (i + 1) * fold_size\n",
    "        folds.append(sorted_reports[start_index:end_index])\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7255ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspectj_folds = k_fold(temp_sort_aspectj, 3)\n",
    "swt_folds = k_fold(temp_sort_swt, 10)\n",
    "tomcat_folds = k_fold(temp_sort_tomcat, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9bcd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse_folds = k_fold(temp_sort_eclipse, 10)\n",
    "birt_folds = k_fold(temp_sort_birt, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7a185",
   "metadata": {},
   "source": [
    "# III. Text Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdaef8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report_texts(report, content_type='stemmed'):\n",
    "    summary_tokens = report.pos_tagged_summary.get(content_type, [])\n",
    "    desc_tokens = report.pos_tagged_description.get(content_type, [])\n",
    "        \n",
    "    # Join tokens with a space to form a single document string\n",
    "    return \" \".join(summary_tokens + desc_tokens)\n",
    "\n",
    "def get_source_text(source_file, content_type: str = 'stemmed') -> str:\n",
    "    \"\"\"Concatenates all stemmed source code features into a single string.\"\"\"\n",
    "    # Assuming SourceFile has pos_tagged and stemmed attributes for these components:\n",
    "    features = []\n",
    "    # Note: Using .get(content_type, []) to safely handle potentially missing keys\n",
    "    features.extend(source_file.comments.get(content_type, []))\n",
    "    features.extend(source_file.class_names.get(content_type, []))\n",
    "    features.extend(source_file.attributes.get(content_type, []))\n",
    "    features.extend(source_file.method_names.get(content_type, []))\n",
    "    features.extend(source_file.variables.get(content_type, []))\n",
    "    features.extend(source_file.file_name.get(content_type, []))\n",
    "    return \" \".join(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d069681",
   "metadata": {},
   "source": [
    "# IV. Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f7fcacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rs_pair(report_fold, source_files):\n",
    "    all_pair = []\n",
    "    for report in report_fold:\n",
    "        for source in source_files.values():\n",
    "            pair = {'report' : report, 'source' : source}\n",
    "            all_pair.append(pair)\n",
    "    return all_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25925a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspectj_pair = [rs_pair(fold, data['aspectj']['source']) for fold in aspectj_folds]\n",
    "swt_pair = [rs_pair(fold, data['swt']['source']) for fold in swt_folds]\n",
    "tomcat_pair = [rs_pair(fold, data['tomcat']['source']) for fold in tomcat_folds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse_pair = [rs_pair(fold, data['eclipse']['source']) for fold in eclipse_folds]\n",
    "birt_pair = [rs_pair(fold, data['birt']['source']) for fold in birt_folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e21c1222",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in aspectj_pair:\n",
    "    for pair in fold:\n",
    "        for path in data['aspectj']['source']:\n",
    "            if(data['aspectj']['source'][path] == pair['source']):\n",
    "                pair['path'] = path\n",
    "                break\n",
    "        if pair['path'] in pair['report'][1].fixed_files:\n",
    "            pair['label'] = 1\n",
    "\n",
    "for fold in swt_pair:\n",
    "    for pair in fold:\n",
    "        for path in data['swt']['source']:\n",
    "            if(data['swt']['source'][path] == pair['source']):\n",
    "                pair['path'] = path\n",
    "                break\n",
    "        if pair['path'] in pair['report'][1].fixed_files:\n",
    "            pair['label'] = 1\n",
    "\n",
    "for fold in tomcat_pair:\n",
    "    for pair in fold:\n",
    "        for path in data['tomcat']['source']:\n",
    "            if(data['tomcat']['source'][path] == pair['source']):\n",
    "                pair['path'] = path\n",
    "                break\n",
    "        if pair['path'] in pair['report'][1].fixed_files:\n",
    "            pair['label'] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in eclipse_pair:\n",
    "    for pair in fold:\n",
    "        for path in data['eclipse']['source']:\n",
    "            if(data['eclipse']['source'][path] == pair['source']):\n",
    "                pair['path'] = path\n",
    "                break\n",
    "        if pair['path'] in pair['report'][1].fixed_files:\n",
    "            pair['label'] = 1\n",
    "\n",
    "for fold in birt_pair:\n",
    "    for pair in fold:\n",
    "        for path in data['birt']['source']:\n",
    "            if(data['birt']['source'][path] == pair['source']):\n",
    "                pair['path'] = path\n",
    "                break\n",
    "        if pair['path'] in pair['report'][1].fixed_files:\n",
    "            pair['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa50a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in aspectj_pair:\n",
    "    for pair in fold:\n",
    "        if 'label' not in pair:\n",
    "            pair['label'] = 0\n",
    "\n",
    "for fold in swt_pair:\n",
    "    for pair in fold:\n",
    "        if 'label' not in pair:\n",
    "            pair['label'] = 0\n",
    "            \n",
    "for fold in tomcat_pair:\n",
    "    for pair in fold:\n",
    "        if 'label' not in pair:\n",
    "            pair['label'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d0b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in eclipse_pair:\n",
    "    for pair in fold:\n",
    "        if 'label' not in pair:\n",
    "            pair['label'] = 0\n",
    "\n",
    "for fold in birt_pair:\n",
    "    for pair in fold:\n",
    "        if 'label' not in pair:\n",
    "            pair['label'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482cfb6a",
   "metadata": {},
   "source": [
    "# V. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2eb8c",
   "metadata": {},
   "source": [
    "### 1. Lexical Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc835f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rs_lexical_vector(fold):\n",
    "    # --- 1. PREPARE CORPUS FOR VECTORIZATION ---\n",
    "    source_corpus = []\n",
    "    report_corpus = []\n",
    "    #Report corpus\n",
    "    report_corpus.append(get_report_texts(fold[0]['report'][1]))\n",
    "    rflag = fold[0]['report'][0]\n",
    "    for i in range(1, len(fold)):\n",
    "        if(fold[i]['report'][0] != rflag):\n",
    "            rflag = fold[i]['report'][0]\n",
    "            report_corpus.append(get_report_texts(fold[i]['report'][1]))\n",
    "    #Source corpus\n",
    "    rflag = fold[0]['report'][0]\n",
    "    count = 1\n",
    "    for i in range(1, len(fold)):\n",
    "        if(fold[i]['report'][0] != rflag):\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "    source_corpus = [get_source_text(fold[i]['source']) for i in range(count)]\n",
    "\n",
    "    corpus = source_corpus + report_corpus    \n",
    "    # Fit a TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\S+',\n",
    "        min_df=10,\n",
    "        max_features=5000\n",
    "    )\n",
    "\n",
    "    tfidf = vectorizer.fit(corpus)\n",
    "    source_tfidf_matrix = tfidf.transform(source_corpus)\n",
    "    report_tfidf_matrix = tfidf.transform(report_corpus)\n",
    "    \n",
    "    for i in range(len(fold)//count):\n",
    "        for j in range(count):\n",
    "            lexical_index = {'source_vector' : j,\n",
    "                              'report_vector' : i}\n",
    "            fold[i*count + j]['lexical'] = lexical_index\n",
    "\n",
    "    return [report_tfidf_matrix, source_tfidf_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce211619",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspecj_lexical_matrix = []\n",
    "for fold in aspectj_pair:\n",
    "    aspecj_lexical_matrix.append(rs_lexical_vector(fold))\n",
    "#\n",
    "swt_lexical_matrix = []\n",
    "for fold in swt_pair:\n",
    "    swt_lexical_matrix.append(rs_lexical_vector(fold))\n",
    "#\n",
    "tomcat_lexical_matrix = []\n",
    "for fold in tomcat_pair:\n",
    "    tomcat_lexical_matrix.append(rs_lexical_vector(fold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse_lexical_matrix = []\n",
    "for fold in eclipse_pair:\n",
    "    eclipse_lexical_matrix.append(rs_lexical_vector(fold))\n",
    "#\n",
    "birt_lexical_matrix = []\n",
    "for fold in birt_pair:\n",
    "     birt_lexical_matrix.append(rs_lexical_vector(fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "453ec762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_similarity(report_vector, source_vector):\n",
    "        sim = cosine_similarity(report_vector.reshape(1, -1),\n",
    "                                source_vector.reshape(1, -1))[0][0]\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7f2dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(aspectj_pair)): #== len(aspectj_lexical_matrix)\n",
    "    for pair in aspectj_pair[i]:\n",
    "        sim = lexical_similarity(aspecj_lexical_matrix[i][0].getrow(pair['lexical']['report_vector']),\n",
    "                                 aspecj_lexical_matrix[i][1].getrow(pair['lexical']['source_vector']))\n",
    "        pair['lexical']['Lsim'] = sim\n",
    "\n",
    "for i in range(len(swt_pair)): #== len(swt_lexical_matrix)\n",
    "    for pair in swt_pair[i]:\n",
    "        sim = lexical_similarity(swt_lexical_matrix[i][0].getrow(pair['lexical']['report_vector']),\n",
    "                                 swt_lexical_matrix[i][1].getrow(pair['lexical']['source_vector']))\n",
    "        pair['lexical']['Lsim'] = sim\n",
    "\n",
    "for i in range(len(tomcat_pair)): #== len(tomcat_lexical_matrix)\n",
    "    for pair in tomcat_pair[i]:\n",
    "        sim = lexical_similarity(tomcat_lexical_matrix[i][0].getrow(pair['lexical']['report_vector']),\n",
    "                                 tomcat_lexical_matrix[i][1].getrow(pair['lexical']['source_vector']))\n",
    "        pair['lexical']['Lsim'] = sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75991578",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eclipse_pair)): #== len(eclipse_lexical_matrix)\n",
    "    for pair in eclipse_pair[i]:\n",
    "        sim = lexical_similarity(eclipse_lexical_matrix[i][0].getrow(pair['lexical']['report_vector']),\n",
    "                                 eclipse_lexical_matrix[i][1].getrow(pair['lexical']['source_vector']))\n",
    "        pair['lexical']['Lsim'] = sim\n",
    "\n",
    "for i in range(len(birt_pair)): #== len(birt_lexical_matrix)\n",
    "    for pair in birt_pair[i]:\n",
    "        sim = lexical_similarity(birt_lexical_matrix[i][0].getrow(pair['lexical']['report_vector']),\n",
    "                                 birt_lexical_matrix[i][1].getrow(pair['lexical']['source_vector']))\n",
    "        pair['lexical']['Lsim'] = sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7cf643",
   "metadata": {},
   "source": [
    "### 2. Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "241018a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_utils.py excerpt\n",
    "import os\n",
    "# !!! IMPORTANT: YOU MUST ADJUST THIS PATH IF YOUR FILE IS ELSEWHERE !!!\n",
    "GLOVE_FILE_PATH = Path.cwd() / 'glove' / 'glove.42B.300d.txt'\n",
    "GLOVE_VECTOR_SIZE = 300\n",
    "class GloVeModel:\n",
    "    \"\"\"\n",
    "    A class to load and manage the GloVe word embeddings.\n",
    "    It provides fast lookup for embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, path=GLOVE_FILE_PATH, vector_size=GLOVE_VECTOR_SIZE):\n",
    "        self.path = path\n",
    "        self.vector_size = vector_size\n",
    "        self.embedding_dict = {}\n",
    "        self.is_loaded = False\n",
    "        self.zero_vector = np.zeros(self.vector_size, dtype=np.float32)\n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"Loads the GloVe embeddings from the file path into a dictionary.\"\"\"\n",
    "        if self.is_loaded:\n",
    "            print(\"GloVe model already loaded.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Loading GloVe embeddings from: {self.path} (Size: {self.vector_size}D)...\")\n",
    "        try:\n",
    "            with open(self.path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    parts = line.split()\n",
    "                    word = parts[0]\n",
    "                    vector = np.array(parts[1:], dtype=np.float32)\n",
    "                    \n",
    "                    if len(vector) == self.vector_size:\n",
    "                        self.embedding_dict[word] = vector\n",
    "\n",
    "            self.is_loaded = True\n",
    "            print(f\"Successfully loaded {len(self.embedding_dict)} words.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"ERROR: GloVe file not found at {self.path}.\")\n",
    "            print(\"Please check the GLOVE_FILE_PATH and ensure the file exists before running.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during GloVe loading: {e}\")\n",
    "\n",
    "    def __getitem__(self, word: str) -> np.ndarray:\n",
    "        \"\"\"Returns the embedding vector for a word or a zero vector if not found.\"\"\"\n",
    "        return self.embedding_dict.get(word, self.zero_vector)\n",
    "    \n",
    "    def __contains__(self, word: str) -> bool:\n",
    "        \"\"\"Checks if a word is in the vocabulary.\"\"\"\n",
    "        return word in self.embedding_dict\n",
    "\n",
    "    def get_vector_size(self) -> int:\n",
    "        \"\"\"Returns the dimension size of the vectors.\"\"\"\n",
    "        return self.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "582c9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_glove_vector(corpus, model):\n",
    "    \"\"\"\n",
    "    Computes the average GloVe vector for a given document text.\n",
    "    \"\"\"        \n",
    "    # Tokenize the text (assuming words are separated by spaces in the corpus strings)\n",
    "    tokens = corpus.split()\n",
    "    \n",
    "    # Filter out words not in the GloVe vocabulary and get their vectors\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    \n",
    "    if not vectors:\n",
    "        # If the document is empty or contains no words in the vocabulary, return a zero vector\n",
    "        return model.zero_vector\n",
    "    \n",
    "    # Calculate the mean (average) of all word vectors\n",
    "    # Stack the vectors vertically and compute the mean along axis 0\n",
    "    return np.mean(np.vstack(vectors), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f71253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings from: d:\\Lab\\3. Deep Learning\\Challenge Task NLP 1 - Bug Localization\\glove\\glove.42B.300d.txt (Size: 300D)...\n",
      "Successfully loaded 1917494 words.\n"
     ]
    }
   ],
   "source": [
    "glove = GloVeModel()\n",
    "glove.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "252591cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rs_glove_vector(fold, glove_model):\n",
    "        # --- 1. PREPARE CORPUS FOR VECTORIZATION ---\n",
    "    source_corpus = []\n",
    "    report_corpus = []\n",
    "    #Report corpus\n",
    "    report_corpus.append(get_report_texts(fold[0]['report'][1]))\n",
    "    rflag = fold[0]['report'][0]\n",
    "    for i in range(1, len(fold)):\n",
    "        if(fold[i]['report'][0] != rflag):\n",
    "            rflag = fold[i]['report'][0]\n",
    "            report_corpus.append(get_report_texts(fold[i]['report'][1]))\n",
    "    #Source corpus\n",
    "    rflag = fold[0]['report'][0]\n",
    "    count = 1\n",
    "    for i in range(1, len(fold)):\n",
    "        if(fold[i]['report'][0] != rflag):\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "    source_corpus = [get_source_text(fold[i]['source']) for i in range(count)]\n",
    "\n",
    "    report_glove_matrix = []\n",
    "    source_glove_matrix = []\n",
    "    for doc in report_corpus:\n",
    "        avg_vector = get_avg_glove_vector(doc, glove_model)\n",
    "        report_glove_matrix.append(avg_vector)\n",
    "    for doc in source_corpus:\n",
    "        avg_vector = get_avg_glove_vector(doc, glove_model)\n",
    "        source_glove_matrix.append(avg_vector)\n",
    "    \n",
    "    return [report_glove_matrix, source_glove_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c53a937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspectj_semantic_matrix = []\n",
    "for fold in aspectj_pair:\n",
    "    aspectj_semantic_matrix.append(rs_glove_vector(fold, glove))\n",
    "\n",
    "swt_semantic_matrix = []\n",
    "for fold in swt_pair:\n",
    "    swt_semantic_matrix.append(rs_glove_vector(fold, glove))\n",
    "\n",
    "tomcat_semantic_matrix = []\n",
    "for fold in tomcat_pair:\n",
    "    tomcat_semantic_matrix.append(rs_glove_vector(fold, glove))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90806ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse_semantic_matrix = []\n",
    "for fold in eclipse_pair:\n",
    "    eclipse_semantic_matrix.append(rs_glove_vector(fold, glove))\n",
    "\n",
    "birt_semantic_matrix = []\n",
    "for fold in birt_pair:\n",
    "    birt_semantic_matrix.append(rs_glove_vector(fold, glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d9d687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity(report_vector, source_vector):\n",
    "    sim = cosine_similarity(report_vector.reshape(1, -1),\n",
    "                            source_vector.reshape(1, -1))[0][0]\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6c5dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(aspectj_pair)): #== len(aspectj_lexical_matrix)\n",
    "    for j in range(len(aspectj_semantic_matrix[i][0])):\n",
    "        for k in range(len(aspectj_semantic_matrix[i][1])):\n",
    "            sim = semantic_similarity(aspectj_semantic_matrix[i][0][j], aspectj_semantic_matrix[i][1][k])\n",
    "            aspectj_pair[i][j*len(aspectj_semantic_matrix[i][1])+k]['Ssim'] = sim\n",
    "\n",
    "for i in range(len(swt_pair)): #== len(swt_lexical_matrix)\n",
    "    for j in range(len(swt_semantic_matrix[i][0])):\n",
    "        for k in range(len(swt_semantic_matrix[i][1])):\n",
    "            sim = semantic_similarity(swt_semantic_matrix[i][0][j], swt_semantic_matrix[i][1][k])\n",
    "            swt_pair[i][j*len(swt_semantic_matrix[i][1])+k]['Ssim'] = sim\n",
    "\n",
    "for i in range(len(tomcat_pair)): #== len(tomcat_lexical_matrix)\n",
    "    for j in range(len(tomcat_semantic_matrix[i][0])):\n",
    "        for k in range(len(tomcat_semantic_matrix[i][1])):\n",
    "            sim = semantic_similarity(tomcat_semantic_matrix[i][0][j], tomcat_semantic_matrix[i][1][k])\n",
    "            tomcat_pair[i][j*len(tomcat_semantic_matrix[i][1])+k]['Ssim'] = sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eclipse_pair)): \n",
    "    for j in range(len(eclipse_semantic_matrix[i][0])):\n",
    "        for k in range(len(eclipse_semantic_matrix[i][1])):\n",
    "            sim = semantic_similarity(eclipse_semantic_matrix[i][0][j], eclipse_semantic_matrix[i][1][k])\n",
    "            eclipse_pair[i][j*len(eclipse_semantic_matrix[i][1])+k]['Ssim'] = sim\n",
    "\n",
    "for i in range(len(birt_pair)): #== len(birt_lexical_matrix)\n",
    "    for j in range(len(birt_semantic_matrix[i][0])):\n",
    "        for k in range(len(birt_semantic_matrix[i][1])):\n",
    "            sim = semantic_similarity(birt_semantic_matrix[i][0][j], birt_semantic_matrix[i][1][k])\n",
    "            birt_pair[i][j*len(birt_semantic_matrix[i][1])+k]['Ssim'] = sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c3d0f2",
   "metadata": {},
   "source": [
    "### Similar Bug Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f01d659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rs_BugReportSim(fold, name):\n",
    "\n",
    "    bug_change = []\n",
    "\n",
    "    for i in range (len(data[name]['source'])):\n",
    "        for j in range (i, len(fold), len(data[name]['source'])):\n",
    "            if fold[j]['label'] == 1:\n",
    "                bug_change.append(fold[j])\n",
    "            else:\n",
    "                fold[j]['R'] = 0\n",
    "        if len(bug_change) > 0:   \n",
    "            R = sum(a['lexical']['Lsim'] for a in bug_change)/len(bug_change)\n",
    "            for a in bug_change:\n",
    "                a['R'] = R\n",
    "        bug_change = []\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23548917",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in aspectj_pair:\n",
    "    rs_BugReportSim(fold, 'aspectj')\n",
    "\n",
    "for fold in swt_pair:\n",
    "    rs_BugReportSim(fold, 'swt')\n",
    "\n",
    "for fold in tomcat_pair:\n",
    "    rs_BugReportSim(fold, 'tomcat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3187a540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'report': ('11280', <preprocessing.BugReport at 0x218e47174c0>),\n",
       " 'source': <preprocessing.SourceFile at 0x218cb12aa40>,\n",
       " 'path': 'ajde\\\\src\\\\org\\\\aspectj\\\\ajde\\\\ui\\\\swing\\\\BrowserStructureViewToolPanel.java',\n",
       " 'label': 0,\n",
       " 'lexical': {'source_vector': 58,\n",
       "  'report_vector': 0,\n",
       "  'Lsim': np.float64(0.15625950259973578)},\n",
       " 'Ssim': np.float32(0.7631036),\n",
       " 'R': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspectj_pair[0][58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in eclipse_pair:\n",
    "    rs_BugReportSim(fold, 'eclipse')\n",
    "\n",
    "for fold in birt_pair:\n",
    "    rs_BugReportSim(fold, 'birt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd561d4",
   "metadata": {},
   "source": [
    "### Code Change History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2021740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = 30\n",
    "def rs_CodeChangeHistory(fold, name):\n",
    "    for i in range(len(data[name]['source'])):\n",
    "        fold[i]['H'] = 0.01\n",
    "\n",
    "    for i in range(len(data[name]['source']), len(fold)):\n",
    "        for j in range(i, -1, -len(data[name]['source'])):\n",
    "            t_dif = (fold[i]['report'][1].report_time - fold[j]['report'][1].report_time).days\n",
    "            if t_dif > u:\n",
    "                break\n",
    "            if fold[j]['label'] == 1:\n",
    "                if t_dif <= u:\n",
    "                    if t_dif == 0:\n",
    "                        t_dif +=1\n",
    "                    H = 1 / t_dif\n",
    "                    fold[i]['H'] = H\n",
    "                    break\n",
    "            fold[i]['H'] = 0.01\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b8a651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in aspectj_pair:\n",
    "    rs_CodeChangeHistory(fold, 'aspectj')\n",
    "\n",
    "for fold in swt_pair:\n",
    "    rs_CodeChangeHistory(fold, \"swt\")\n",
    "\n",
    "for fold in tomcat_pair:\n",
    "    rs_CodeChangeHistory(fold, 'tomcat')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in eclipse_pair:\n",
    "    rs_CodeChangeHistory(fold, 'eclipse')\n",
    "\n",
    "for fold in birt_pair:\n",
    "    rs_CodeChangeHistory(fold, 'birt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95581390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d25e13b6",
   "metadata": {},
   "source": [
    "### Bug Fixing Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "863c593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rs_BugFixFreg(fold, name):\n",
    "    for i in range(len(data[name]['source'])):\n",
    "        fold[i]['F'] = 0\n",
    "    for i in range(len(data[name]['source']), len(fold)):\n",
    "        count = 0\n",
    "        for j in range(i, -1, -len(data[name]['source'])):\n",
    "            if fold[j]['label'] == 1:\n",
    "                count = count +1\n",
    "        \n",
    "        fold[i]['F'] = count\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16ca8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in aspectj_pair:\n",
    "    rs_BugFixFreg(fold, 'aspectj')\n",
    "\n",
    "for fold in swt_pair:\n",
    "    rs_BugFixFreg(fold, 'swt')\n",
    "\n",
    "for fold in tomcat_pair:\n",
    "    rs_BugFixFreg(fold, 'tomcat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f70237",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in eclipse_pair:\n",
    "    rs_BugFixFreg(fold, 'eclipse')\n",
    "\n",
    "for fold in birt_pair:\n",
    "    rs_BugFixFreg(fold, 'birt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e23a8e",
   "metadata": {},
   "source": [
    "### CNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f04e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3b366b4",
   "metadata": {},
   "source": [
    "# VI. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d2d8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMax(fold):\n",
    "    a = np.array([pair['lexical']['Lsim'] for pair in fold]).reshape(-1,1)\n",
    "    b = np.array([pair['Ssim'] for pair in fold]).reshape(-1, 1)\n",
    "    c = np.array([pair['R'] for pair in fold]).reshape(-1, 1)\n",
    "    d = np.array([pair['H'] for pair in fold]).reshape(-1, 1)\n",
    "    e = np.array([pair['F'] for pair in fold]).reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    a = scaler.fit_transform(a)\n",
    "    b = scaler.fit_transform(b)\n",
    "    c = scaler.fit_transform(c)\n",
    "    d = scaler.fit_transform(d)\n",
    "    e = scaler.fit_transform(e)\n",
    "    for i in range(len(fold)):\n",
    "        ma = a[i].item()\n",
    "        mb = b[i].item()\n",
    "        mc = c[i].item()\n",
    "        md = d[i].item()\n",
    "        me = e[i].item()\n",
    "        fold[i]['scaled_vector'] = [ma, mb, mc, md, me]       \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b11ec004",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in aspectj_pair:\n",
    "    MinMax(fold)\n",
    "\n",
    "for fold in swt_pair:\n",
    "    MinMax(fold)\n",
    "\n",
    "for fold in tomcat_pair:\n",
    "    MinMax(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3795ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in eclipse_pair:\n",
    "    MinMax(fold)\n",
    "\n",
    "for fold in birt_pair:\n",
    "    MinMax(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b38bf",
   "metadata": {},
   "source": [
    "# VII. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9e61e",
   "metadata": {},
   "source": [
    "Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "804c69b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "random.seed(time.time())\n",
    "batch_size = 128\n",
    "def batching(fold):\n",
    "    neg = []\n",
    "    pos = []\n",
    "    for pair in fold:\n",
    "        if pair['label'] == 0:\n",
    "            neg.append(pair)\n",
    "        else:\n",
    "            pos.append(pair)\n",
    "\n",
    "    k = len(neg)//8\n",
    "\n",
    "    X = []\n",
    "    size1 = len(neg) // k\n",
    "    remainder1 = len(neg)%k\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for i in range(k):\n",
    "        end = start + size1 + (1 if i < remainder1 else 0)\n",
    "        X.append(neg[start:end])\n",
    "        start = end\n",
    "        \n",
    "\n",
    "    pos1 = pos.copy()\n",
    "    random.shuffle(pos1)\n",
    "    size2 = batch_size - size1\n",
    "    start = 0\n",
    "    end = 0\n",
    "    remainder2 = len(pos1) %k\n",
    "    for j in range(k):\n",
    "        end = start + size2 + (1 if j < remainder2 else 0)\n",
    "        X[j].extend(pos1[start:end])\n",
    "        start = end\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546e7c8",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8624fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction = 'mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        inputs: predicted probabilities (after sigmoid), shape (batch, 1)\n",
    "        targets: ground truth labels (0 or 1), shape (batch, 1)\n",
    "        \"\"\"\n",
    "        # Clip to avoid log(0)\n",
    "        eps = 1e-7\n",
    "        inputs = torch.clamp(inputs, eps, 1.0 - eps)\n",
    "\n",
    "        # Compute cross entropy\n",
    "        ce_loss = - (targets * torch.log(inputs) + (1 - targets) * torch.log(1 - inputs))\n",
    "\n",
    "        # Compute pt (probability of the true class)\n",
    "        pt = torch.where(targets == 1, inputs, 1 - inputs)\n",
    "\n",
    "        # Apply focal loss modulation\n",
    "        focal_weight = (self.alpha * targets + (1 - self.alpha) * (1 - targets)) * (1 - pt) ** self.gamma\n",
    "        loss = focal_weight * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "    \n",
    "\n",
    "# Define the NN model\n",
    "class BugLocNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BugLocNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 300)\n",
    "        self.fc2 = nn.Linear(300, 150)\n",
    "        self.fc3 = nn.Linear(150, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfadf98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_model(train_fold, epochs):\n",
    "    # Build batches from training fold\n",
    "    batches = batching(train_fold)\n",
    "\n",
    "    # Model, optimizer, loss\n",
    "    model = BugLocNN(input_dim=5)  # 5-dim vectors\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in batches:\n",
    "            batch_X = torch.tensor([pair['scaled_vector'] for pair in batch])\n",
    "            batch_y = torch.tensor([pair['label'] for pair in batch]).unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(batches):.10f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_fold):\n",
    "    with torch.no_grad():\n",
    "        # Convert test fold into tensors\n",
    "        X_test = torch.tensor([pair['scaled_vector'] for pair in test_fold], dtype=torch.float32)\n",
    "\n",
    "        # Get predictions\n",
    "        preds = model(X_test)                      # probabilities\n",
    "\n",
    "    return preds.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "47543488",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batching(aspectj_pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "67d00ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch[0][0]['scaled_vector'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810fb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.0001047072\n",
      "Epoch 2/30, Loss: 0.0000167460\n",
      "Epoch 3/30, Loss: 0.0000164482\n",
      "Epoch 4/30, Loss: 0.0000162323\n",
      "Epoch 5/30, Loss: 0.0000160140\n",
      "Epoch 6/30, Loss: 0.0000157875\n",
      "Epoch 7/30, Loss: 0.0000155548\n",
      "Epoch 8/30, Loss: 0.0000153173\n",
      "Epoch 9/30, Loss: 0.0000150766\n",
      "Epoch 10/30, Loss: 0.0000148328\n",
      "Epoch 11/30, Loss: 0.0000145855\n",
      "Epoch 12/30, Loss: 0.0000143344\n",
      "Epoch 13/30, Loss: 0.0000140793\n",
      "Epoch 14/30, Loss: 0.0000138201\n",
      "Epoch 15/30, Loss: 0.0000135566\n",
      "Epoch 16/30, Loss: 0.0000132883\n",
      "Epoch 17/30, Loss: 0.0000130146\n",
      "Epoch 18/30, Loss: 0.0000127357\n",
      "Epoch 19/30, Loss: 0.0000124515\n",
      "Epoch 20/30, Loss: 0.0000121606\n",
      "Epoch 21/30, Loss: 0.0000118627\n",
      "Epoch 22/30, Loss: 0.0000115574\n",
      "Epoch 23/30, Loss: 0.0000112449\n",
      "Epoch 24/30, Loss: 0.0000109250\n",
      "Epoch 25/30, Loss: 0.0000105976\n",
      "Epoch 26/30, Loss: 0.0000102621\n",
      "Epoch 27/30, Loss: 0.0000099181\n",
      "Epoch 28/30, Loss: 0.0000095653\n",
      "Epoch 29/30, Loss: 0.0000092029\n",
      "Epoch 30/30, Loss: 0.0000088320\n",
      "Epoch 1/30, Loss: 0.0001010613\n",
      "Epoch 2/30, Loss: 0.0000167324\n",
      "Epoch 3/30, Loss: 0.0000164128\n",
      "Epoch 4/30, Loss: 0.0000162006\n",
      "Epoch 5/30, Loss: 0.0000159904\n",
      "Epoch 6/30, Loss: 0.0000157724\n",
      "Epoch 7/30, Loss: 0.0000155482\n",
      "Epoch 8/30, Loss: 0.0000153193\n",
      "Epoch 9/30, Loss: 0.0000150858\n",
      "Epoch 10/30, Loss: 0.0000148483\n",
      "Epoch 11/30, Loss: 0.0000146067\n",
      "Epoch 12/30, Loss: 0.0000143615\n",
      "Epoch 13/30, Loss: 0.0000141134\n",
      "Epoch 14/30, Loss: 0.0000138619\n",
      "Epoch 15/30, Loss: 0.0000136067\n",
      "Epoch 16/30, Loss: 0.0000133474\n",
      "Epoch 17/30, Loss: 0.0000130836\n",
      "Epoch 18/30, Loss: 0.0000128148\n",
      "Epoch 19/30, Loss: 0.0000125415\n",
      "Epoch 20/30, Loss: 0.0000122635\n",
      "Epoch 21/30, Loss: 0.0000119811\n",
      "Epoch 22/30, Loss: 0.0000116930\n",
      "Epoch 23/30, Loss: 0.0000113991\n",
      "Epoch 24/30, Loss: 0.0000110992\n",
      "Epoch 25/30, Loss: 0.0000107927\n",
      "Epoch 26/30, Loss: 0.0000104791\n",
      "Epoch 27/30, Loss: 0.0000101587\n",
      "Epoch 28/30, Loss: 0.0000098312\n",
      "Epoch 29/30, Loss: 0.0000094972\n",
      "Epoch 30/30, Loss: 0.0000091570\n"
     ]
    }
   ],
   "source": [
    "#Aspectj\n",
    "for i in range(len(aspectj_pair)):\n",
    "    if i == len(aspectj_pair) - 1:\n",
    "        break\n",
    "    model = train_model(aspectj_pair[i], 30)\n",
    "    pred = test_model(model, aspectj_pair[i+1])\n",
    "    for j in range (len(aspectj_pair[i+1])):\n",
    "        aspectj_pair[i+1][j]['pred'] = pred[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e475570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.0001811172\n",
      "Epoch 2/15, Loss: 0.0000394441\n",
      "Epoch 3/15, Loss: 0.0000389755\n",
      "Epoch 4/15, Loss: 0.0000383912\n",
      "Epoch 5/15, Loss: 0.0000377332\n",
      "Epoch 6/15, Loss: 0.0000370409\n",
      "Epoch 7/15, Loss: 0.0000363323\n",
      "Epoch 8/15, Loss: 0.0000356125\n",
      "Epoch 9/15, Loss: 0.0000348829\n",
      "Epoch 10/15, Loss: 0.0000341440\n",
      "Epoch 11/15, Loss: 0.0000333959\n",
      "Epoch 12/15, Loss: 0.0000326401\n",
      "Epoch 13/15, Loss: 0.0000318733\n",
      "Epoch 14/15, Loss: 0.0000310907\n",
      "Epoch 15/15, Loss: 0.0000302906\n",
      "Epoch 1/15, Loss: 0.0001346292\n",
      "Epoch 2/15, Loss: 0.0000352337\n",
      "Epoch 3/15, Loss: 0.0000346691\n",
      "Epoch 4/15, Loss: 0.0000340130\n",
      "Epoch 5/15, Loss: 0.0000332890\n",
      "Epoch 6/15, Loss: 0.0000325278\n",
      "Epoch 7/15, Loss: 0.0000317441\n",
      "Epoch 8/15, Loss: 0.0000309442\n",
      "Epoch 9/15, Loss: 0.0000301289\n",
      "Epoch 10/15, Loss: 0.0000292974\n",
      "Epoch 11/15, Loss: 0.0000284484\n",
      "Epoch 12/15, Loss: 0.0000275816\n",
      "Epoch 13/15, Loss: 0.0000266955\n",
      "Epoch 14/15, Loss: 0.0000257907\n",
      "Epoch 15/15, Loss: 0.0000248681\n",
      "Epoch 1/15, Loss: 0.0001704041\n",
      "Epoch 2/15, Loss: 0.0000374056\n",
      "Epoch 3/15, Loss: 0.0000368992\n",
      "Epoch 4/15, Loss: 0.0000362464\n",
      "Epoch 5/15, Loss: 0.0000355139\n",
      "Epoch 6/15, Loss: 0.0000347470\n",
      "Epoch 7/15, Loss: 0.0000339604\n",
      "Epoch 8/15, Loss: 0.0000331572\n",
      "Epoch 9/15, Loss: 0.0000323380\n",
      "Epoch 10/15, Loss: 0.0000315039\n",
      "Epoch 11/15, Loss: 0.0000306600\n",
      "Epoch 12/15, Loss: 0.0000297968\n",
      "Epoch 13/15, Loss: 0.0000289126\n",
      "Epoch 14/15, Loss: 0.0000280049\n",
      "Epoch 15/15, Loss: 0.0000270727\n",
      "Epoch 1/15, Loss: 0.0001491655\n",
      "Epoch 2/15, Loss: 0.0000349483\n",
      "Epoch 3/15, Loss: 0.0000344427\n",
      "Epoch 4/15, Loss: 0.0000337790\n",
      "Epoch 5/15, Loss: 0.0000330504\n",
      "Epoch 6/15, Loss: 0.0000322907\n",
      "Epoch 7/15, Loss: 0.0000315103\n",
      "Epoch 8/15, Loss: 0.0000307135\n",
      "Epoch 9/15, Loss: 0.0000299014\n",
      "Epoch 10/15, Loss: 0.0000290738\n",
      "Epoch 11/15, Loss: 0.0000282286\n",
      "Epoch 12/15, Loss: 0.0000273638\n",
      "Epoch 13/15, Loss: 0.0000264789\n",
      "Epoch 14/15, Loss: 0.0000255738\n",
      "Epoch 15/15, Loss: 0.0000246499\n",
      "Epoch 1/15, Loss: 0.0001843759\n",
      "Epoch 2/15, Loss: 0.0000349472\n",
      "Epoch 3/15, Loss: 0.0000341522\n",
      "Epoch 4/15, Loss: 0.0000332944\n",
      "Epoch 5/15, Loss: 0.0000323683\n",
      "Epoch 6/15, Loss: 0.0000313995\n",
      "Epoch 7/15, Loss: 0.0000303995\n",
      "Epoch 8/15, Loss: 0.0000293750\n",
      "Epoch 9/15, Loss: 0.0000283258\n",
      "Epoch 10/15, Loss: 0.0000272517\n",
      "Epoch 11/15, Loss: 0.0000261533\n",
      "Epoch 12/15, Loss: 0.0000250345\n",
      "Epoch 13/15, Loss: 0.0000239017\n",
      "Epoch 14/15, Loss: 0.0000227568\n",
      "Epoch 15/15, Loss: 0.0000216045\n",
      "Epoch 1/15, Loss: 0.0001445542\n",
      "Epoch 2/15, Loss: 0.0000316898\n",
      "Epoch 3/15, Loss: 0.0000310753\n",
      "Epoch 4/15, Loss: 0.0000303610\n",
      "Epoch 5/15, Loss: 0.0000295708\n",
      "Epoch 6/15, Loss: 0.0000287356\n",
      "Epoch 7/15, Loss: 0.0000278723\n",
      "Epoch 8/15, Loss: 0.0000269895\n",
      "Epoch 9/15, Loss: 0.0000260874\n",
      "Epoch 10/15, Loss: 0.0000251689\n",
      "Epoch 11/15, Loss: 0.0000242361\n",
      "Epoch 12/15, Loss: 0.0000232932\n",
      "Epoch 13/15, Loss: 0.0000223403\n",
      "Epoch 14/15, Loss: 0.0000213831\n",
      "Epoch 15/15, Loss: 0.0000204318\n",
      "Epoch 1/15, Loss: 0.0001616406\n",
      "Epoch 2/15, Loss: 0.0000363976\n",
      "Epoch 3/15, Loss: 0.0000359309\n",
      "Epoch 4/15, Loss: 0.0000354088\n",
      "Epoch 5/15, Loss: 0.0000348254\n",
      "Epoch 6/15, Loss: 0.0000342083\n",
      "Epoch 7/15, Loss: 0.0000335730\n",
      "Epoch 8/15, Loss: 0.0000329292\n",
      "Epoch 9/15, Loss: 0.0000322819\n",
      "Epoch 10/15, Loss: 0.0000316311\n",
      "Epoch 11/15, Loss: 0.0000309749\n",
      "Epoch 12/15, Loss: 0.0000303091\n",
      "Epoch 13/15, Loss: 0.0000296323\n",
      "Epoch 14/15, Loss: 0.0000289438\n",
      "Epoch 15/15, Loss: 0.0000282411\n",
      "Epoch 1/15, Loss: 0.0001828449\n",
      "Epoch 2/15, Loss: 0.0000352671\n",
      "Epoch 3/15, Loss: 0.0000347808\n",
      "Epoch 4/15, Loss: 0.0000342134\n",
      "Epoch 5/15, Loss: 0.0000335807\n",
      "Epoch 6/15, Loss: 0.0000329122\n",
      "Epoch 7/15, Loss: 0.0000322217\n",
      "Epoch 8/15, Loss: 0.0000315144\n",
      "Epoch 9/15, Loss: 0.0000307926\n",
      "Epoch 10/15, Loss: 0.0000300559\n",
      "Epoch 11/15, Loss: 0.0000293032\n",
      "Epoch 12/15, Loss: 0.0000285341\n",
      "Epoch 13/15, Loss: 0.0000277505\n",
      "Epoch 14/15, Loss: 0.0000269526\n",
      "Epoch 15/15, Loss: 0.0000261416\n",
      "Epoch 1/15, Loss: 0.0001673235\n",
      "Epoch 2/15, Loss: 0.0000368598\n",
      "Epoch 3/15, Loss: 0.0000363353\n",
      "Epoch 4/15, Loss: 0.0000357546\n",
      "Epoch 5/15, Loss: 0.0000351115\n",
      "Epoch 6/15, Loss: 0.0000344342\n",
      "Epoch 7/15, Loss: 0.0000337375\n",
      "Epoch 8/15, Loss: 0.0000330278\n",
      "Epoch 9/15, Loss: 0.0000323078\n",
      "Epoch 10/15, Loss: 0.0000315782\n",
      "Epoch 11/15, Loss: 0.0000308379\n",
      "Epoch 12/15, Loss: 0.0000300861\n",
      "Epoch 13/15, Loss: 0.0000293212\n",
      "Epoch 14/15, Loss: 0.0000285408\n",
      "Epoch 15/15, Loss: 0.0000277447\n"
     ]
    }
   ],
   "source": [
    "#SWT\n",
    "for i in range(len(swt_pair)):\n",
    "    if i == len(swt_pair) - 1:\n",
    "        break\n",
    "    model = train_model(swt_pair[i], 15)\n",
    "    pred = test_model(model, swt_pair[i+1])\n",
    "    for j in range (len(swt_pair[i+1])):\n",
    "        swt_pair[i+1][j]['pred'] = pred[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e6c56bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0005820078\n",
      "Epoch 2/10, Loss: 0.0000474056\n",
      "Epoch 3/10, Loss: 0.0000429258\n",
      "Epoch 4/10, Loss: 0.0000414368\n",
      "Epoch 5/10, Loss: 0.0000406791\n",
      "Epoch 6/10, Loss: 0.0000401763\n",
      "Epoch 7/10, Loss: 0.0000397780\n",
      "Epoch 8/10, Loss: 0.0000394263\n",
      "Epoch 9/10, Loss: 0.0000390963\n",
      "Epoch 10/10, Loss: 0.0000387759\n",
      "Epoch 1/10, Loss: 0.0006225382\n",
      "Epoch 2/10, Loss: 0.0000582961\n",
      "Epoch 3/10, Loss: 0.0000543935\n",
      "Epoch 4/10, Loss: 0.0000531039\n",
      "Epoch 5/10, Loss: 0.0000524197\n",
      "Epoch 6/10, Loss: 0.0000519333\n",
      "Epoch 7/10, Loss: 0.0000515221\n",
      "Epoch 8/10, Loss: 0.0000511407\n",
      "Epoch 9/10, Loss: 0.0000507700\n",
      "Epoch 10/10, Loss: 0.0000504020\n",
      "Epoch 1/10, Loss: 0.0006695734\n",
      "Epoch 2/10, Loss: 0.0000540170\n",
      "Epoch 3/10, Loss: 0.0000495139\n",
      "Epoch 4/10, Loss: 0.0000480346\n",
      "Epoch 5/10, Loss: 0.0000472733\n",
      "Epoch 6/10, Loss: 0.0000467554\n",
      "Epoch 7/10, Loss: 0.0000463345\n",
      "Epoch 8/10, Loss: 0.0000459550\n",
      "Epoch 9/10, Loss: 0.0000455940\n",
      "Epoch 10/10, Loss: 0.0000452406\n",
      "Epoch 1/10, Loss: 0.0004513016\n",
      "Epoch 2/10, Loss: 0.0000535641\n",
      "Epoch 3/10, Loss: 0.0000498682\n",
      "Epoch 4/10, Loss: 0.0000486350\n",
      "Epoch 5/10, Loss: 0.0000479699\n",
      "Epoch 6/10, Loss: 0.0000474900\n",
      "Epoch 7/10, Loss: 0.0000470800\n",
      "Epoch 8/10, Loss: 0.0000466982\n",
      "Epoch 9/10, Loss: 0.0000463272\n",
      "Epoch 10/10, Loss: 0.0000459599\n",
      "Epoch 1/10, Loss: 0.0006911753\n",
      "Epoch 2/10, Loss: 0.0000548410\n",
      "Epoch 3/10, Loss: 0.0000508887\n",
      "Epoch 4/10, Loss: 0.0000495841\n",
      "Epoch 5/10, Loss: 0.0000488978\n",
      "Epoch 6/10, Loss: 0.0000484164\n",
      "Epoch 7/10, Loss: 0.0000480140\n",
      "Epoch 8/10, Loss: 0.0000476442\n",
      "Epoch 9/10, Loss: 0.0000472876\n",
      "Epoch 10/10, Loss: 0.0000469353\n",
      "Epoch 1/10, Loss: 0.0006437894\n",
      "Epoch 2/10, Loss: 0.0000560780\n",
      "Epoch 3/10, Loss: 0.0000510401\n",
      "Epoch 4/10, Loss: 0.0000493959\n",
      "Epoch 5/10, Loss: 0.0000485682\n",
      "Epoch 6/10, Loss: 0.0000480236\n",
      "Epoch 7/10, Loss: 0.0000475952\n",
      "Epoch 8/10, Loss: 0.0000472185\n",
      "Epoch 9/10, Loss: 0.0000468654\n",
      "Epoch 10/10, Loss: 0.0000465230\n",
      "Epoch 1/10, Loss: 0.0006704979\n",
      "Epoch 2/10, Loss: 0.0000519775\n",
      "Epoch 3/10, Loss: 0.0000474441\n",
      "Epoch 4/10, Loss: 0.0000459288\n",
      "Epoch 5/10, Loss: 0.0000451369\n",
      "Epoch 6/10, Loss: 0.0000445942\n",
      "Epoch 7/10, Loss: 0.0000441521\n",
      "Epoch 8/10, Loss: 0.0000437534\n",
      "Epoch 9/10, Loss: 0.0000433736\n",
      "Epoch 10/10, Loss: 0.0000430013\n",
      "Epoch 1/10, Loss: 0.0007544080\n",
      "Epoch 2/10, Loss: 0.0000582680\n",
      "Epoch 3/10, Loss: 0.0000535859\n",
      "Epoch 4/10, Loss: 0.0000520733\n",
      "Epoch 5/10, Loss: 0.0000513073\n",
      "Epoch 6/10, Loss: 0.0000507942\n",
      "Epoch 7/10, Loss: 0.0000503808\n",
      "Epoch 8/10, Loss: 0.0000500104\n",
      "Epoch 9/10, Loss: 0.0000496585\n",
      "Epoch 10/10, Loss: 0.0000493147\n",
      "Epoch 1/10, Loss: 0.0006648959\n",
      "Epoch 2/10, Loss: 0.0000551313\n",
      "Epoch 3/10, Loss: 0.0000508946\n",
      "Epoch 4/10, Loss: 0.0000495030\n",
      "Epoch 5/10, Loss: 0.0000487725\n",
      "Epoch 6/10, Loss: 0.0000482599\n",
      "Epoch 7/10, Loss: 0.0000478309\n",
      "Epoch 8/10, Loss: 0.0000474361\n",
      "Epoch 9/10, Loss: 0.0000470557\n",
      "Epoch 10/10, Loss: 0.0000466806\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tomcat_pair)):\n",
    "    if i == len(tomcat_pair) - 1:\n",
    "        break\n",
    "    model = train_model(tomcat_pair[i], 10)\n",
    "    pred = test_model(model, tomcat_pair[i+1])\n",
    "    for j in range (len(tomcat_pair[i+1])):\n",
    "        tomcat_pair[i+1][j]['pred'] = pred[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d6a4b",
   "metadata": {},
   "source": [
    "# VIII. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b076e3a",
   "metadata": {},
   "source": [
    "Top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "43af2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score\n",
    "def TopK(set, name):\n",
    "    n = {1: 0, 5: 0, 10 : 0, 15:0}\n",
    "    n1 = 0\n",
    "    n5 = 0\n",
    "    n10 = 0\n",
    "    n15 = 0\n",
    "    step = len(data[name]['source'])\n",
    "    count = 0\n",
    "    for i in range(1, len(set)):\n",
    "        p = len(set[i])\n",
    "        for j in range(0, p, step):\n",
    "            count +=1\n",
    "            y = [pair for pair in set[i][j:j+step]]\n",
    "            y.sort(key = lambda x: x['pred'], reverse = True)\n",
    "            \n",
    "            #top1\n",
    "            # Top-1\n",
    "            if any(pair['label'] == 1 for pair in y[:1]):\n",
    "                n1 += 1\n",
    "\n",
    "            # Top-5\n",
    "            if any(pair['label'] == 1 for pair in y[:5]):\n",
    "                n5 += 1\n",
    "\n",
    "            # Top-10\n",
    "            if any(pair['label'] == 1 for pair in y[:10]):\n",
    "                n10 += 1\n",
    "\n",
    "            # Top-15\n",
    "            if any(pair['label'] == 1 for pair in y[:15]):\n",
    "                n15 += 1\n",
    "    n[1] = n1/count\n",
    "    n[5] = n5/count\n",
    "    n[10] = n10/count\n",
    "    n[15] = n15/count\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af47c577",
   "metadata": {},
   "source": [
    "MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a217d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRR(folds, name):\n",
    "    step = len(data[name]['source'])   # number of candidate files per bug report\n",
    "    count = 0\n",
    "    rr_sum = 0.0                  # sum of reciprocal ranks\n",
    "\n",
    "    for fold in folds[1:]:  # skip fold 0 if it's training\n",
    "        for j in range(0, len(fold), step):\n",
    "            count += 1\n",
    "            y = fold[j:j+step]\n",
    "\n",
    "            # sort candidates by prediction score descending\n",
    "            y.sort(key=lambda x: x['pred'], reverse=True)\n",
    "\n",
    "            # find the rank of the first buggy file (label == 1)\n",
    "            rank = None\n",
    "            for idx, pair in enumerate(y, start=1):\n",
    "                if pair['label'] == 1:   # dataset ground truth\n",
    "                    rank = idx\n",
    "                    break\n",
    "\n",
    "            # reciprocal rank contribution\n",
    "            if rank is not None:\n",
    "                rr_sum += 1.0 / rank\n",
    "            else:\n",
    "                rr_sum += 0.0  # no buggy file found\n",
    "\n",
    "    # mean reciprocal rank\n",
    "    mrr = rr_sum / count if count > 0 else 0.0\n",
    "    return mrr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe746f",
   "metadata": {},
   "source": [
    "MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "969b6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAP(folds, name):\n",
    "    step = len(data[name]['source'])   # number of candidate files per bug report\n",
    "    count = 0\n",
    "    ap_sum = 0.0\n",
    "\n",
    "    for fold in folds[1:]:  # skip fold 0 if it's training\n",
    "        pairs = fold   # convert OrderedDict to list\n",
    "        for j in range(0, len(pairs), step):\n",
    "            count += 1\n",
    "            y = pairs[j:j+step]\n",
    "\n",
    "            # sort by prediction score descending\n",
    "            y.sort(key=lambda x: x['pred'], reverse=True)\n",
    "\n",
    "            # total relevant files for this bug report\n",
    "            R = sum(pair['label'] == 1 for pair in y)\n",
    "            if R == 0:\n",
    "                continue  # skip if no relevant files\n",
    "\n",
    "            precisions = []\n",
    "            relevant_found = 0\n",
    "            for idx, pair in enumerate(y, start=1):\n",
    "                if pair['label'] == 1:\n",
    "                    relevant_found += 1\n",
    "                    precisions.append(relevant_found / idx)\n",
    "\n",
    "            # average precision for this bug report\n",
    "            ap_sum += sum(precisions) / R\n",
    "\n",
    "    # mean average precision\n",
    "    map_score = ap_sum / count if count > 0 else 0.0\n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "275e0bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(aspectj_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ccecc4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4349969345603154"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr_swt = MRR(swt_pair, 'swt')\n",
    "mrr_swt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b587d1",
   "metadata": {},
   "source": [
    "Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f45ea39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_aspectj = TopK(aspectj_pair, 'aspectj')\n",
    "topk_swt = TopK(swt_pair, 'swt')\n",
    "topk_tomcat = TopK(tomcat_pair, 'tomcat')\n",
    "\n",
    "mrr_aspectj = MRR(aspectj_pair, 'aspectj')\n",
    "mrr_swt = MRR(swt_pair, 'swt')\n",
    "mrr_tomcat = MRR(tomcat_pair, 'tomcat')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a6356f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008038184618329334"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr_tomcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bb350ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0010976948408342481,\n",
       " 5: 0.0010976948408342481,\n",
       " 10: 0.0010976948408342481,\n",
       " 15: 0.003293084522502744}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_tomcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "34f6547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_aspectj = MAP(aspectj_pair, 'aspectj')\n",
    "map_swt = MAP(swt_pair, 'swt')\n",
    "map_tomcat = MAP(tomcat_pair, 'tomcat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a04915a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008038184618329334"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_tomcat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
